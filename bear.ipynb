{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abhijeet_Assignment6_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfprogress/daily/blob/master/bear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqYz1bN5Z5Kb",
        "colab_type": "text"
      },
      "source": [
        "Abhijeet Singh  \n",
        "EIP-3 Phase-2 Session-2 Assignment  \n",
        "www.absingh.com\n",
        "\n",
        "Changes:\n",
        "- Add encoding to file read\n",
        "- Remove all punctuation from source text\n",
        "- Remove DropOut from layer before Dense\n",
        "- Add dropout to input LSTM layer\n",
        "- Convert input to padded sequences\n",
        "- Predict 500 characters\n",
        "- DropOut of 0.1 everywhere\n",
        "- Train for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X57UEp3OcPqo",
        "colab_type": "code",
        "outputId": "5ad1af34-e28e-458a-b72b-4d8bd08e0d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_H2fFeYJw4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/cseas/img/master/wonderland.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHTqJdJXN-Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUmGMZj3UcrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, encoding='utf-8-sig').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L6raqeAYXNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove punctuation symbols from text\n",
        "import string\n",
        "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
        "raw_text = raw_text.translate(translate_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzgbaK45Uw88",
        "colab_type": "code",
        "outputId": "d68ccd52-7f38-455f-dce4-6c3d8db44d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '0', '3', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uznW0obNU2nj",
        "colab_type": "code",
        "outputId": "1de6c27d-d1a8-4500-a3d6-5547c5c20542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136111\n",
            "Total Vocab:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyjwN40kU9Xn",
        "colab_type": "code",
        "outputId": "9768219c-0a16-41a2-a968-64629c02ca36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 20\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  136011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWIzQz96DBGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to padded sequences\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "dataX = pad_sequences(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esMluekeXbBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PEExyBTX4AE",
        "colab_type": "code",
        "outputId": "e0505d98-2a24-4a06-a49f-3ff50a57c494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), \n",
        "               dropout=0.1, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 30)                7710      \n",
            "=================================================================\n",
            "Total params: 797,214\n",
            "Trainable params: 797,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs1E4NVZcEUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "\n",
        "# Prepare model saving directory.\n",
        "save_dir = \"/content/gdrive/My Drive/models/\"\n",
        "model_name = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "filepath = save_dir + model_name\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV9uxaGsdBVr",
        "colab_type": "code",
        "outputId": "75155caa-c4ca-49b5-c5b3-3a534aa311ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "136011/136011 [==============================] - 577s 4ms/step - loss: 2.6364\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.63640, saving model to /content/gdrive/My Drive/models/weights-improvement-01-2.6364-bigger.hdf5\n",
            "Epoch 2/100\n",
            "136011/136011 [==============================] - 571s 4ms/step - loss: 2.2333\n",
            "\n",
            "Epoch 00002: loss improved from 2.63640 to 2.23335, saving model to /content/gdrive/My Drive/models/weights-improvement-02-2.2333-bigger.hdf5\n",
            "Epoch 3/100\n",
            "136011/136011 [==============================] - 570s 4ms/step - loss: 2.0344\n",
            "\n",
            "Epoch 00003: loss improved from 2.23335 to 2.03440, saving model to /content/gdrive/My Drive/models/weights-improvement-03-2.0344-bigger.hdf5\n",
            "Epoch 4/100\n",
            "136011/136011 [==============================] - 566s 4ms/step - loss: 1.9049\n",
            "\n",
            "Epoch 00004: loss improved from 2.03440 to 1.90493, saving model to /content/gdrive/My Drive/models/weights-improvement-04-1.9049-bigger.hdf5\n",
            "Epoch 5/100\n",
            "136011/136011 [==============================] - 567s 4ms/step - loss: 1.8103\n",
            "\n",
            "Epoch 00005: loss improved from 1.90493 to 1.81034, saving model to /content/gdrive/My Drive/models/weights-improvement-05-1.8103-bigger.hdf5\n",
            "Epoch 6/100\n",
            "136011/136011 [==============================] - 566s 4ms/step - loss: 1.7365\n",
            "\n",
            "Epoch 00006: loss improved from 1.81034 to 1.73651, saving model to /content/gdrive/My Drive/models/weights-improvement-06-1.7365-bigger.hdf5\n",
            "Epoch 7/100\n",
            "136011/136011 [==============================] - 571s 4ms/step - loss: 1.6725\n",
            "\n",
            "Epoch 00007: loss improved from 1.73651 to 1.67252, saving model to /content/gdrive/My Drive/models/weights-improvement-07-1.6725-bigger.hdf5\n",
            "Epoch 8/100\n",
            "136011/136011 [==============================] - 570s 4ms/step - loss: 1.6229\n",
            "\n",
            "Epoch 00008: loss improved from 1.67252 to 1.62287, saving model to /content/gdrive/My Drive/models/weights-improvement-08-1.6229-bigger.hdf5\n",
            "Epoch 9/100\n",
            "136011/136011 [==============================] - 571s 4ms/step - loss: 1.5775\n",
            "\n",
            "Epoch 00009: loss improved from 1.62287 to 1.57751, saving model to /content/gdrive/My Drive/models/weights-improvement-09-1.5775-bigger.hdf5\n",
            "Epoch 10/100\n",
            "136011/136011 [==============================] - 572s 4ms/step - loss: 1.5327\n",
            "\n",
            "Epoch 00010: loss improved from 1.57751 to 1.53266, saving model to /content/gdrive/My Drive/models/weights-improvement-10-1.5327-bigger.hdf5\n",
            "Epoch 11/100\n",
            "136011/136011 [==============================] - 575s 4ms/step - loss: 1.4948\n",
            "\n",
            "Epoch 00011: loss improved from 1.53266 to 1.49476, saving model to /content/gdrive/My Drive/models/weights-improvement-11-1.4948-bigger.hdf5\n",
            "Epoch 12/100\n",
            "136011/136011 [==============================] - 572s 4ms/step - loss: 1.4603\n",
            "\n",
            "Epoch 00012: loss improved from 1.49476 to 1.46032, saving model to /content/gdrive/My Drive/models/weights-improvement-12-1.4603-bigger.hdf5\n",
            "Epoch 13/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.4273\n",
            "\n",
            "Epoch 00013: loss improved from 1.46032 to 1.42729, saving model to /content/gdrive/My Drive/models/weights-improvement-13-1.4273-bigger.hdf5\n",
            "Epoch 14/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.3994\n",
            "\n",
            "Epoch 00014: loss improved from 1.42729 to 1.39945, saving model to /content/gdrive/My Drive/models/weights-improvement-14-1.3994-bigger.hdf5\n",
            "Epoch 15/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.3712\n",
            "\n",
            "Epoch 00015: loss improved from 1.39945 to 1.37121, saving model to /content/gdrive/My Drive/models/weights-improvement-15-1.3712-bigger.hdf5\n",
            "Epoch 16/100\n",
            "136011/136011 [==============================] - 575s 4ms/step - loss: 1.3464\n",
            "\n",
            "Epoch 00016: loss improved from 1.37121 to 1.34637, saving model to /content/gdrive/My Drive/models/weights-improvement-16-1.3464-bigger.hdf5\n",
            "Epoch 17/100\n",
            "136011/136011 [==============================] - 574s 4ms/step - loss: 1.3195\n",
            "\n",
            "Epoch 00017: loss improved from 1.34637 to 1.31946, saving model to /content/gdrive/My Drive/models/weights-improvement-17-1.3195-bigger.hdf5\n",
            "Epoch 18/100\n",
            "136011/136011 [==============================] - 574s 4ms/step - loss: 1.2983\n",
            "\n",
            "Epoch 00018: loss improved from 1.31946 to 1.29828, saving model to /content/gdrive/My Drive/models/weights-improvement-18-1.2983-bigger.hdf5\n",
            "Epoch 19/100\n",
            "136011/136011 [==============================] - 574s 4ms/step - loss: 1.2797\n",
            "\n",
            "Epoch 00019: loss improved from 1.29828 to 1.27972, saving model to /content/gdrive/My Drive/models/weights-improvement-19-1.2797-bigger.hdf5\n",
            "Epoch 20/100\n",
            "136011/136011 [==============================] - 572s 4ms/step - loss: 1.2587\n",
            "\n",
            "Epoch 00020: loss improved from 1.27972 to 1.25874, saving model to /content/gdrive/My Drive/models/weights-improvement-20-1.2587-bigger.hdf5\n",
            "Epoch 21/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.2398\n",
            "\n",
            "Epoch 00021: loss improved from 1.25874 to 1.23976, saving model to /content/gdrive/My Drive/models/weights-improvement-21-1.2398-bigger.hdf5\n",
            "Epoch 22/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.2225\n",
            "\n",
            "Epoch 00022: loss improved from 1.23976 to 1.22249, saving model to /content/gdrive/My Drive/models/weights-improvement-22-1.2225-bigger.hdf5\n",
            "Epoch 23/100\n",
            "136011/136011 [==============================] - 572s 4ms/step - loss: 1.2077\n",
            "\n",
            "Epoch 00023: loss improved from 1.22249 to 1.20773, saving model to /content/gdrive/My Drive/models/weights-improvement-23-1.2077-bigger.hdf5\n",
            "Epoch 24/100\n",
            "136011/136011 [==============================] - 571s 4ms/step - loss: 1.1903\n",
            "\n",
            "Epoch 00024: loss improved from 1.20773 to 1.19027, saving model to /content/gdrive/My Drive/models/weights-improvement-24-1.1903-bigger.hdf5\n",
            "Epoch 25/100\n",
            "136011/136011 [==============================] - 570s 4ms/step - loss: 1.1731\n",
            "\n",
            "Epoch 00025: loss improved from 1.19027 to 1.17308, saving model to /content/gdrive/My Drive/models/weights-improvement-25-1.1731-bigger.hdf5\n",
            "Epoch 26/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.1592\n",
            "\n",
            "Epoch 00026: loss improved from 1.17308 to 1.15917, saving model to /content/gdrive/My Drive/models/weights-improvement-26-1.1592-bigger.hdf5\n",
            "Epoch 27/100\n",
            "136011/136011 [==============================] - 574s 4ms/step - loss: 1.1495\n",
            "\n",
            "Epoch 00027: loss improved from 1.15917 to 1.14952, saving model to /content/gdrive/My Drive/models/weights-improvement-27-1.1495-bigger.hdf5\n",
            "Epoch 28/100\n",
            "136011/136011 [==============================] - 573s 4ms/step - loss: 1.1319\n",
            "\n",
            "Epoch 00028: loss improved from 1.14952 to 1.13186, saving model to /content/gdrive/My Drive/models/weights-improvement-28-1.1319-bigger.hdf5\n",
            "Epoch 29/100\n",
            "102336/136011 [=====================>........] - ETA: 2:21 - loss: 1.1125Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ksfpl8ebXD",
        "colab_type": "code",
        "outputId": "82f38323-62b2-4e08-e538-d787cd9a3018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Output truncated, last saved model is of epoch 63, continue training from epoch 64\n",
        "# load the model\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/gdrive/My Drive/models/weights-improvement-63-0.9213-bigger.hdf5\")\n",
        "# then fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list, initial_epoch=63)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0723 15:41:52.429114 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0723 15:41:52.470895 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0723 15:41:52.478215 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0723 15:41:52.763219 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0723 15:41:52.776000 139684304533376 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0723 15:41:53.431963 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0723 15:41:56.477463 139684304533376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0723 15:41:56.649035 139684304533376 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64/100\n",
            "136011/136011 [==============================] - 845s 6ms/step - loss: 0.9185\n",
            "\n",
            "Epoch 00064: loss improved from inf to 0.91849, saving model to /content/gdrive/My Drive/models/weights-improvement-64-0.9185-bigger.hdf5\n",
            "Epoch 65/100\n",
            "136011/136011 [==============================] - 849s 6ms/step - loss: 0.9212\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.91849\n",
            "Epoch 66/100\n",
            "136011/136011 [==============================] - 848s 6ms/step - loss: 0.9138\n",
            "\n",
            "Epoch 00066: loss improved from 0.91849 to 0.91384, saving model to /content/gdrive/My Drive/models/weights-improvement-66-0.9138-bigger.hdf5\n",
            "Epoch 67/100\n",
            "136011/136011 [==============================] - 853s 6ms/step - loss: 0.9160\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.91384\n",
            "Epoch 68/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.9075\n",
            "\n",
            "Epoch 00068: loss improved from 0.91384 to 0.90747, saving model to /content/gdrive/My Drive/models/weights-improvement-68-0.9075-bigger.hdf5\n",
            "Epoch 69/100\n",
            "136011/136011 [==============================] - 852s 6ms/step - loss: 0.9099\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.90747\n",
            "Epoch 70/100\n",
            "136011/136011 [==============================] - 849s 6ms/step - loss: 0.9051\n",
            "\n",
            "Epoch 00070: loss improved from 0.90747 to 0.90508, saving model to /content/gdrive/My Drive/models/weights-improvement-70-0.9051-bigger.hdf5\n",
            "Epoch 71/100\n",
            "136011/136011 [==============================] - 851s 6ms/step - loss: 0.9023\n",
            "\n",
            "Epoch 00071: loss improved from 0.90508 to 0.90233, saving model to /content/gdrive/My Drive/models/weights-improvement-71-0.9023-bigger.hdf5\n",
            "Epoch 72/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.9071\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.90233\n",
            "Epoch 73/100\n",
            "136011/136011 [==============================] - 853s 6ms/step - loss: 0.9061\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.90233\n",
            "Epoch 74/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.8990\n",
            "\n",
            "Epoch 00074: loss improved from 0.90233 to 0.89898, saving model to /content/gdrive/My Drive/models/weights-improvement-74-0.8990-bigger.hdf5\n",
            "Epoch 75/100\n",
            "136011/136011 [==============================] - 858s 6ms/step - loss: 0.8989\n",
            "\n",
            "Epoch 00075: loss improved from 0.89898 to 0.89891, saving model to /content/gdrive/My Drive/models/weights-improvement-75-0.8989-bigger.hdf5\n",
            "Epoch 76/100\n",
            "136011/136011 [==============================] - 862s 6ms/step - loss: 0.9031\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.89891\n",
            "Epoch 77/100\n",
            "136011/136011 [==============================] - 856s 6ms/step - loss: 0.8950\n",
            "\n",
            "Epoch 00077: loss improved from 0.89891 to 0.89495, saving model to /content/gdrive/My Drive/models/weights-improvement-77-0.8950-bigger.hdf5\n",
            "Epoch 78/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.8960\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.89495\n",
            "Epoch 79/100\n",
            "136011/136011 [==============================] - 852s 6ms/step - loss: 0.9002\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.89495\n",
            "Epoch 80/100\n",
            "136011/136011 [==============================] - 856s 6ms/step - loss: 0.8967\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.89495\n",
            "Epoch 81/100\n",
            "136011/136011 [==============================] - 856s 6ms/step - loss: 0.8906\n",
            "\n",
            "Epoch 00081: loss improved from 0.89495 to 0.89063, saving model to /content/gdrive/My Drive/models/weights-improvement-81-0.8906-bigger.hdf5\n",
            "Epoch 82/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.8908\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.89063\n",
            "Epoch 83/100\n",
            "136011/136011 [==============================] - 853s 6ms/step - loss: 0.8875\n",
            "\n",
            "Epoch 00083: loss improved from 0.89063 to 0.88755, saving model to /content/gdrive/My Drive/models/weights-improvement-83-0.8875-bigger.hdf5\n",
            "Epoch 84/100\n",
            "136011/136011 [==============================] - 858s 6ms/step - loss: 0.8951\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.88755\n",
            "Epoch 85/100\n",
            "136011/136011 [==============================] - 856s 6ms/step - loss: 0.8856\n",
            "\n",
            "Epoch 00085: loss improved from 0.88755 to 0.88563, saving model to /content/gdrive/My Drive/models/weights-improvement-85-0.8856-bigger.hdf5\n",
            "Epoch 86/100\n",
            "136011/136011 [==============================] - 858s 6ms/step - loss: 0.8781\n",
            "\n",
            "Epoch 00086: loss improved from 0.88563 to 0.87813, saving model to /content/gdrive/My Drive/models/weights-improvement-86-0.8781-bigger.hdf5\n",
            "Epoch 87/100\n",
            "136011/136011 [==============================] - 854s 6ms/step - loss: 0.8800\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.87813\n",
            "Epoch 88/100\n",
            "136011/136011 [==============================] - 855s 6ms/step - loss: 0.8871\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.87813\n",
            "Epoch 89/100\n",
            "136011/136011 [==============================] - 854s 6ms/step - loss: 0.8823\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.87813\n",
            "Epoch 90/100\n",
            "136011/136011 [==============================] - 854s 6ms/step - loss: 0.8911\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.87813\n",
            "Epoch 91/100\n",
            "136011/136011 [==============================] - 851s 6ms/step - loss: 0.8791\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.87813\n",
            "Epoch 92/100\n",
            " 97088/136011 [====================>.........] - ETA: 4:02 - loss: 0.8834Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVMtSj_X6WS5",
        "colab_type": "code",
        "outputId": "c640b1c7-492a-4757-aebb-90e429bd346e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "# Output truncated, last saved model is of epoch 97, continue training from epoch 98\n",
        "# load the model\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/gdrive/My Drive/models/weights-improvement-97-0.8514-bigger.hdf5\")\n",
        "# then fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list, initial_epoch=97)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 07:38:59.171493 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 07:38:59.220469 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 07:38:59.228467 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 07:38:59.478618 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0724 07:38:59.497540 140019712563072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0724 07:39:00.142180 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0724 07:39:03.415584 140019712563072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0724 07:39:03.578190 140019712563072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 98/100\n",
            "136011/136011 [==============================] - 1840s 14ms/step - loss: 0.8700\n",
            "\n",
            "Epoch 00098: loss improved from inf to 0.86995, saving model to /content/gdrive/My Drive/models/weights-improvement-98-0.8700-bigger.hdf5\n",
            "Epoch 99/100\n",
            "136011/136011 [==============================] - 1587s 12ms/step - loss: 0.8767\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.86995\n",
            "Epoch 100/100\n",
            "136011/136011 [==============================] - 1442s 11ms/step - loss: 0.8899\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.86995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f58969b7908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU378inJ2uzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-97-0.8514-bigger.hdf5\"\n",
        "model.load_weights(save_dir + filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OUcuRPH3C65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668GK0qE3N--",
        "colab_type": "code",
        "outputId": "0b2e2e9b-190a-4205-a6f0-87fe91216c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  \n",
        "  # \tpattern.append(index)\n",
        "  pattern = numpy.append(pattern, index)\n",
        "  \t\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" \n",
            "a bit\n",
            "\n",
            "perhaps it hasnt one alice ventured to remark\n",
            "\n",
            "tut tut child said the duchess everythings go \"\n",
            "r the waid the hatter waid to the caterpillar \n",
            "the said the waid to here teadling the the whong the whought ill the was aod the hed to tee whe muckn ard the had good th tole of the tab it tas io a foers to sie said alice \n",
            "she ooee turtle sape oe tee roo the hedd to the sea ot the whought alice \n",
            "she ouee turtle sighed deao so tee of course wo the tea it taid alice \n",
            "she hors a little baler anl a farters topesting it at she said to herself as \n",
            "the corm taid the hatter whe sueen sail of tha said to \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XqS0uZrkw8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}